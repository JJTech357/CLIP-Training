{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6704e42",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85249574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "boston_pd = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "boston_pd = boston_pd.fillna(boston_pd.mean())\n",
    "\n",
    "new_column_names = {0: 'x1', 1: 'x2', 2: 'x3', 3: 'x4', 4: 'x5', 5: 'x6', 6: 'x7', 7: 'x8', 8: 'x9', 9: 'x10', 10: 'target'}\n",
    "\n",
    "boston_pd.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "boston_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into data and label arrays \n",
    "y = boston_pd['target']\n",
    "X = boston_pd.drop(['target'], axis=1)\n",
    "\n",
    "# create training (~80%) and test data sets\n",
    "X_train = X[:400]\n",
    "X_test = X[400:]\n",
    "y_train = y[:400]\n",
    "y_test = y[400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49f7f2",
   "metadata": {},
   "source": [
    "# Single Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed750f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a LinearRegression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Measure the time it takes to fit the model\n",
    "start_time = time.time()\n",
    "model = lr.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken for fitting\n",
    "fit_time = end_time - start_time\n",
    "print(f\"Fitting Time: {fit_time} seconds\")\n",
    "\n",
    "# Measure the time it takes to make predictions\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the time taken for prediction\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Prediction Time: {predict_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deacc08",
   "metadata": {},
   "source": [
    "# Native Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf703754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Regression\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c02585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to a Spark data frame\n",
    "boston_sp = spark.createDataFrame(boston_pd)\n",
    "#display(boston_sp.take(5))\n",
    "\n",
    "# split into training and test spark data frames\n",
    "boston_train = spark.createDataFrame(boston_pd[:400])\n",
    "boston_test = spark.createDataFrame(boston_pd[400:])\n",
    "\n",
    "# convert to vector representation for MLlib\n",
    "assembler = VectorAssembler(inputCols= boston_train.schema.names[:(boston_pd.shape[1] - 1)],  \n",
    "                                                                        outputCol=\"features\" )\n",
    "boston_train = assembler.transform(boston_train).select('features', 'target') \n",
    "boston_test = assembler.transform(boston_test).select('features', 'target') \n",
    "\n",
    "# display(boston_train.take(5))\n",
    "\n",
    "# linear regresion with Spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# linear regression \n",
    "lr = LinearRegression(maxIter=10, regParam=0.1, \n",
    "                      elasticNetParam=0.5, labelCol=\"target\")\n",
    "\n",
    "# Fit the model\n",
    "start_time = time.time()\n",
    "model = lr.fit(boston_train)\n",
    "end_time = time.time()\n",
    "fit_time = end_time - start_time\n",
    "print(f\"Fitting Time: {fit_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "boston_pred = model.transform(boston_test)\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Prediction Time: {predict_time} seconds\")\n",
    "\n",
    "# calculate results \n",
    "r = boston_pred.stat.corr(\"prediction\", \"target\")\n",
    "print(\"R-squared: \" + str(r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f636980",
   "metadata": {},
   "source": [
    "Source Code: https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3557c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/organizations-2000000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Founded', 'Number of employees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Founded': 'x', 'Number of employees': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "# data = [(row['x'], row['y']) for _, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e48d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/linear_regression_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c65682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'X': 'x', 'Y': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39692c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/12 15:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/12 15:41:38 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 19.382373094558716 seconds\n",
      "Linear Regression Model: y = -1.3283691617368033e+242 * x + -2.0057847463742946e+241\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"RDDLinearRegression\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "data = [(row['x'], row['y']) for _, row in df.iterrows()]\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "# Define the number of iterations and learning rate\n",
    "num_iterations = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the weights (slope and intercept)\n",
    "weights = (0.0, 0.0)\n",
    "\n",
    "# Perform gradient descent to learn the linear regression coefficients\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    # Compute the gradients for the weights\n",
    "    gradients = data_rdd.map(lambda data_point: (\n",
    "        -2 * data_point[0] * (data_point[1] - (weights[0] * data_point[0] + weights[1])),\n",
    "        -2 * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "    )).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    weights = (weights[0] - learning_rate * gradients[0], weights[1] - learning_rate * gradients[1])\n",
    "\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Training Time: {predict_time} seconds\")\n",
    "\n",
    "# The final weights represent the learned linear regression coefficients\n",
    "slope, intercept = weights\n",
    "print(f\"Linear Regression Model: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f37206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 6.557524919509888 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1.3283691617368033e+242, -2.0057847463742946e+241)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load your CSV data into a Pandas DataFrame\n",
    "#df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "\n",
    "# Convert the DataFrame to a list of tuples\n",
    "#data = [(row['x'], row['y']) for _, row in df.iterrows()]\n",
    "\n",
    "# Define the number of iterations and learning rate\n",
    "num_iterations = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the weights (slope and intercept)\n",
    "weights = (0.0, 0.0)\n",
    "\n",
    "# Perform gradient descent to learn the linear regression coefficients\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    # Compute the gradients for the weights\n",
    "    gradient_sum = [0.0, 0.0]\n",
    "    for data_point in data:\n",
    "        gradient_sum[0] += -2 * data_point[0] * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "        gradient_sum[1] += -2 * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    weights = (weights[0] - learning_rate * gradient_sum[0], weights[1] - learning_rate * gradient_sum[1])\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "\n",
    "# The final weights represent the learned linear regression coefficients\n",
    "slope, intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa397649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 15:45:45 WARN TaskSetManager: Stage 0 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:45 WARN TaskSetManager: Stage 1 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:46 WARN TaskSetManager: Stage 2 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:46 WARN TaskSetManager: Stage 3 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:46 WARN TaskSetManager: Stage 4 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:47 WARN TaskSetManager: Stage 5 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:47 WARN TaskSetManager: Stage 6 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:47 WARN TaskSetManager: Stage 7 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:48 WARN TaskSetManager: Stage 8 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:48 WARN TaskSetManager: Stage 9 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:48 WARN TaskSetManager: Stage 10 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:49 WARN TaskSetManager: Stage 11 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:49 WARN TaskSetManager: Stage 12 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:49 WARN TaskSetManager: Stage 13 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:50 WARN TaskSetManager: Stage 14 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:50 WARN TaskSetManager: Stage 15 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:50 WARN TaskSetManager: Stage 16 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:50 WARN TaskSetManager: Stage 17 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:51 WARN TaskSetManager: Stage 18 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:51 WARN TaskSetManager: Stage 19 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:52 WARN TaskSetManager: Stage 20 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:52 WARN TaskSetManager: Stage 21 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:52 WARN TaskSetManager: Stage 22 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:52 WARN TaskSetManager: Stage 23 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:53 WARN TaskSetManager: Stage 24 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:53 WARN TaskSetManager: Stage 25 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:54 WARN TaskSetManager: Stage 26 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:54 WARN TaskSetManager: Stage 27 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:54 WARN TaskSetManager: Stage 28 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:55 WARN TaskSetManager: Stage 29 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:55 WARN TaskSetManager: Stage 30 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:55 WARN TaskSetManager: Stage 31 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:56 WARN TaskSetManager: Stage 32 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:56 WARN TaskSetManager: Stage 33 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:56 WARN TaskSetManager: Stage 34 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:57 WARN TaskSetManager: Stage 35 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:57 WARN TaskSetManager: Stage 36 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:57 WARN TaskSetManager: Stage 37 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:58 WARN TaskSetManager: Stage 38 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:58 WARN TaskSetManager: Stage 39 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:58 WARN TaskSetManager: Stage 40 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:59 WARN TaskSetManager: Stage 41 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:59 WARN TaskSetManager: Stage 42 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:59 WARN TaskSetManager: Stage 43 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:45:59 WARN TaskSetManager: Stage 44 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:46:00 WARN TaskSetManager: Stage 45 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:46:00 WARN TaskSetManager: Stage 46 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:46:01 WARN TaskSetManager: Stage 47 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:46:01 WARN TaskSetManager: Stage 48 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:46:01 WARN TaskSetManager: Stage 49 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 17.205900192260742 seconds\n",
      "Linear Regression Model: y = -1.328369161736794e+242 * x + -2.0057847463742934e+241\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"RDDLinearRegression2\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "data = [(row['x'], row['y']) for _, row in df.iterrows()]\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "# Define the number of iterations and learning rate\n",
    "num_iterations = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the weights (slope and intercept)\n",
    "weights = (0.0, 0.0)\n",
    "\n",
    "# Perform gradient descent to learn the linear regression coefficients\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    # Compute the gradients for the weights\n",
    "    gradients = data_rdd.map(lambda data_point: (\n",
    "        -2 * data_point[0] * (data_point[1] - (weights[0] * data_point[0] + weights[1])),\n",
    "        -2 * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "    )).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    weights = (weights[0] - learning_rate * gradients[0], weights[1] - learning_rate * gradients[1])\n",
    "\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Training Time: {predict_time} seconds\")\n",
    "\n",
    "# The final weights represent the learned linear regression coefficients\n",
    "slope, intercept = weights\n",
    "print(f\"Linear Regression Model: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034024d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 15:47:42 WARN TaskSetManager: Stage 0 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:43 WARN TaskSetManager: Stage 1 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:43 WARN TaskSetManager: Stage 2 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:44 WARN TaskSetManager: Stage 3 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:44 WARN TaskSetManager: Stage 4 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:45 WARN TaskSetManager: Stage 5 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:45 WARN TaskSetManager: Stage 6 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:45 WARN TaskSetManager: Stage 7 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:46 WARN TaskSetManager: Stage 8 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:46 WARN TaskSetManager: Stage 9 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:46 WARN TaskSetManager: Stage 10 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:47 WARN TaskSetManager: Stage 11 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:47 WARN TaskSetManager: Stage 12 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:47 WARN TaskSetManager: Stage 13 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:48 WARN TaskSetManager: Stage 14 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:48 WARN TaskSetManager: Stage 15 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:48 WARN TaskSetManager: Stage 16 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:49 WARN TaskSetManager: Stage 17 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:49 WARN TaskSetManager: Stage 18 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:49 WARN TaskSetManager: Stage 19 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:50 WARN TaskSetManager: Stage 20 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:50 WARN TaskSetManager: Stage 21 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:51 WARN TaskSetManager: Stage 22 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:51 WARN TaskSetManager: Stage 23 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:51 WARN TaskSetManager: Stage 24 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:52 WARN TaskSetManager: Stage 25 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:52 WARN TaskSetManager: Stage 26 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:52 WARN TaskSetManager: Stage 27 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:53 WARN TaskSetManager: Stage 28 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:53 WARN TaskSetManager: Stage 29 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:53 WARN TaskSetManager: Stage 30 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:54 WARN TaskSetManager: Stage 31 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:54 WARN TaskSetManager: Stage 32 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:54 WARN TaskSetManager: Stage 33 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:55 WARN TaskSetManager: Stage 34 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:55 WARN TaskSetManager: Stage 35 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:56 WARN TaskSetManager: Stage 36 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:56 WARN TaskSetManager: Stage 37 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:56 WARN TaskSetManager: Stage 38 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:57 WARN TaskSetManager: Stage 39 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:57 WARN TaskSetManager: Stage 40 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:57 WARN TaskSetManager: Stage 41 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:58 WARN TaskSetManager: Stage 42 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:58 WARN TaskSetManager: Stage 43 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:58 WARN TaskSetManager: Stage 44 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:59 WARN TaskSetManager: Stage 45 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:59 WARN TaskSetManager: Stage 46 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:47:59 WARN TaskSetManager: Stage 47 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:48:00 WARN TaskSetManager: Stage 48 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/09/12 15:48:00 WARN TaskSetManager: Stage 49 contains a task of very large size (3924 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 18.465419054031372 seconds\n",
      "Linear Regression Model: y = -1.3283691617368772e+242 * x + -2.0057847463744057e+241\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"RDDLinearRegression3\").setMaster(\"local[1]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "data = [(row['x'], row['y']) for _, row in df.iterrows()]\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "# Define the number of iterations and learning rate\n",
    "num_iterations = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the weights (slope and intercept)\n",
    "weights = (0.0, 0.0)\n",
    "\n",
    "# Perform gradient descent to learn the linear regression coefficients\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    # Compute the gradients for the weights\n",
    "    gradients = data_rdd.map(lambda data_point: (\n",
    "        -2 * data_point[0] * (data_point[1] - (weights[0] * data_point[0] + weights[1])),\n",
    "        -2 * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "    )).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    weights = (weights[0] - learning_rate * gradients[0], weights[1] - learning_rate * gradients[1])\n",
    "\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Training Time: {predict_time} seconds\")\n",
    "\n",
    "# The final weights represent the learned linear regression coefficients\n",
    "slope, intercept = weights\n",
    "print(f\"Linear Regression Model: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9becad6",
   "metadata": {},
   "source": [
    "- 1 cores: 18 s\n",
    "- 4 cores: 17 s\n",
    "- 7 cores: 17 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b33434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 17.27907681465149 seconds\n",
      "Linear Regression Model: y = -1.328369161736819e+242 * x + -2.0057847463743227e+241\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"RDDLinearRegression2\").setMaster(\"local[7]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#df = pd.read_csv(\"/Users/muhamadsyukron/Main Folder/Mac 2023 Files/DeepNeuron/HPC_DL_Collab_Lab/data.csv\")\n",
    "data = [(row['x'], row['y']) for _, row in df.iterrows()]\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "# Define the number of iterations and learning rate\n",
    "num_iterations = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the weights (slope and intercept)\n",
    "weights = (0.0, 0.0)\n",
    "\n",
    "# Perform gradient descent to learn the linear regression coefficients\n",
    "start_time = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    # Compute the gradients for the weights\n",
    "    gradients = data_rdd.map(lambda data_point: (\n",
    "        -2 * data_point[0] * (data_point[1] - (weights[0] * data_point[0] + weights[1])),\n",
    "        -2 * (data_point[1] - (weights[0] * data_point[0] + weights[1]))\n",
    "    )).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "    # Update the weights using the gradients and learning rate\n",
    "    weights = (weights[0] - learning_rate * gradients[0], weights[1] - learning_rate * gradients[1])\n",
    "\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Training Time: {predict_time} seconds\")\n",
    "\n",
    "# The final weights represent the learned linear regression coefficients\n",
    "slope, intercept = weights\n",
    "print(f\"Linear Regression Model: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "# y = mx + b\n",
    "# E = -2/n sum(yi - (mxi + b))\n",
    "\n",
    "\n",
    "def grad_desc(m_now, b_now, data, L):\n",
    "    m_grad = 0\n",
    "    b_grad = 0\n",
    "\n",
    "    N = len(data)\n",
    "    for i in range(N):\n",
    "        x = data.iloc[i].x\n",
    "        y = data.iloc[i].y\n",
    "\n",
    "        m_grad += -(2 / N) * x * (y - (m_now * x + b_now))\n",
    "        b_grad += -(2 / N) * (y - (m_now * x + b_now))\n",
    "    m = m_now - m_grad * L\n",
    "    b = b_now - b_grad * L\n",
    "    return m, b\n",
    "\n",
    "\n",
    "m = 0\n",
    "b = 0\n",
    "L = 0.0001\n",
    "epochs = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    m, b = grad_desc(m, b, data, L)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "predict_time = end_time - start_time\n",
    "print(f\"Training Time: {predict_time} seconds\")\n",
    "\n",
    "# plt.scatter(data.x, data.y, color=\"black\")\n",
    "# plt.plot(list(range(50)), ([m * x + b for x in range(50)]), color=\"red\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
